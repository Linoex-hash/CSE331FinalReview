
%%%%%%%%%%%%%% Author: Chandra Neppalli %%%%%%%%%%%%%%%
%%%%%%%%%%%%%% Class: CSE 331 at SUNY University at Buffalo %%%%%%%%%%%%%%

\documentclass{beamer}
\usepackage{dirtytalk}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usetheme{Frankfurt}
\title{CSE 331 Final Exam Preparation}
\subtitle{This is in no way a substitute for exam preperation, mearly a compilation of all the key talking points.}
\author{Chandra Neppalli}
\date{\today}
\newcommand{\defineFigure}[4]{
    \begin{figure}[h]
        \centering
        \includegraphics[width=#2\textwidth]{#1}
        \caption{#3}
        \label{fig:#4}
    \end{figure}
}
\newcommand{\defineFigureReflected}[4]{
    \begin{figure}[h]
        \centering
        \reflectbox{\includegraphics[width=#2\textwidth]{#1}}
        \caption{#3}
        \label{fig:#4}
    \end{figure}
}
\begin{document}
    \maketitle
    \section{Basic Proof Techniques}
    \begin{frame}{Counter Example}  
        \onslide<1->Best proof to use to \textit{disprove} universally true propositions.
        \bigskip

        \onslide<2->Ex: Every day is a Wednesday, where a counter example would be Monday is not Wednesday.
    \end{frame}
    \begin{frame}{Contradiction}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \onslide<1->{Best proof to use if you want to assert something is true.}
                \bigskip
        
                \onslide<2->{Assume what you want to prove is false, then show this leads to a contradiction.}
                \bigskip
        
                \onslide<3->{Therefore, the original assumption has to be true.}             
            \end{column}
            \begin{column}{0.5\textwidth}
                \defineFigureReflected
                {contradiction}{0.75}
                {Sourced from Google}{phoenixwright}
            \end{column}
        \end{columns}
           
    \end{frame}
    \begin{frame}{Contraposition}
        \onslide<1->Best proof for proving causality. Define two propositions E and F.
        \bigskip

        \onslide<2->{If you want to prove that E $\rightarrow$ F, 
        it might be more doable to prove $\lnot$F $\rightarrow$ $\lnot$E, 
        as they are both logically equivalent.}
        \bigskip

        \onslide<3->This is especially useful if the \textbf{scope} of F is smaller than the scope of E.
    \end{frame}
    \begin{frame}{Direct Proof}\label{frame:directproof}
        \onslide<1->If the proof is \textit{simple}, consider directly proving it.
        \bigskip

        \onslide<2->{Remember though, that you must maintain $W.L.O.G$, 
        that your proof can never be too specific and must be arbitrary.}   
    \end{frame}
    \begin{frame}{Proof by Induction}
        \onslide<1->{Proof by Induction is a really nice proof technique when you reduce your proof to a known 
        correct base case.}
        \bigskip

        \onslide<2->{If proof needs to be correct for all numbers $\in \mathbb{N}$, and each step is dependant on the previous step, then \textit{every}
        step can be reduced to a definitive base case that is easy to \hyperlink<2->{frame:directproof}{directly prove}.}
    \end{frame}
    \begin{frame}{Extra: Progress Measure}
        \onslide<1->This is useful for proving an algorithm with a loop terminates.
        \bigskip

        \onslide<2->Let $P(i)$ denote an integer such that:
        \begin{itemize}
            \item<3-> $P(0) = l$ 
            \item<4-> $P(i)$ is an accumulator. This means that $P(i + 1) > P(i)$
            \item<5-> $\forall i,\;\; P(i) \leq k$
        \end{itemize}
        \onslide<6->From these 3 properties, the number of iterations is bounded by $k - l + 1$
        \bigskip

        \onslide<7->Note: This isn't a runtime analysis, rather a proof that the algorithm terminates.
    \end{frame}
    \begin{frame}{Greedy Stays Ahead} \label{def:greedystaysahead}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \onslide<1->{This technique is used to prove that a greedy algorithm returns an optimal solution.}
                \bigskip
        
                \onslide<2->{At every step of a greedy algorithm, it will stay \textit{at least} as far as the optimal solution at that step.}
                \bigskip
        
                \onslide<3->{HW4 \say{Attack on Alarms} and Interval Scheduling are examples of problems with greedy solutions.}            
            \end{column}
            \begin{column}{0.5\textwidth}
                \defineFigure
                {download}{0.85}
                {Sourced from New Grounds}{fig:greedystaysahead}
            \end{column}
        \end{columns}

    \end{frame}
    \section{The Stable Matching Problem}
    \begin{frame}{Introduction}
        \onslide<1->Let's say there are two groups: Group A and Group B.
        \bigskip

        \onslide<2->How do we generate a \textbf{stable} matching between each member of the two groups efficiently?
        \bigskip

        \onslide<3->Moreover, what is a \textbf{stable} matching?
    \end{frame}
    \begin{frame}{Perfect Matchings}
        \onslide<1->A \textbf{perfect matching} is a bijective matching between A and B.
        \bigskip

        \onslide<2->\textbf{Every member} in group A is matched with exactly one member in group B.
        \bigskip

        \onslide<3->Conversely, every member in group B is matched with \textbf{exactly} one member in group A.
        \bigskip

        \onslide<4->With $n$ members in each group, there are $n!$ perfect matchings.
    \end{frame}
    \begin{frame}{Instability}
        \onslide<1->For a particular matching, define a member $m$ from group A and $n$ from group B such that ($m,n$) is not in the matching.
        \bigskip

        \onslide<2->If $m$ prefers $n$ over their current matching \textbf{and} $n$ prefers $m$ over their current matching, then ($m,n$) is an instability to the perfect matching.
    \end{frame}
    \begin{frame}{Stable Matching}
        \only<-5>{
            \onslide<1->A stable matching \textbf{is} a perfect matching with no instabilities.
            \bigskip

            \onslide<2->It therefore follows that the number of stable matchings 
            is \textit{at most} the number of perfect matchings, or $n!$
            \bigskip

            \onslide<3->The Gale Shapely Algorithm is an $O(n^3)$ time algorithm that can output a stable matching.
            \bigskip

            \onslide<4->With the right data structures, the runtime can be reduced to $O(n^2)$. 
            \bigskip

            \onslide<5->Even though the runtime isn't linear, because the input size is $2n^2 \rightarrow \Theta(n^2)$ \only<5->{\footnote{This comes from $n$ Group A members and $n$ Group B members with their $2n$ preference lists}}, the runtime \textbf{with respect} to the input size is $O(N)$, or linear time.
        }
        \only<6>{
            {\small Code}:
            \rule{\textwidth}{0.5pt}
            \texttt{\footnotesize Initially all $m \in M$ and $w \in W$ are free\\
                While there is a man $m$ who is free and hasn’t proposed to
                every woman\\
                \hspace{1cm}Choose such a man $m$\\
                \hspace{1cm}Let $w$ be the highest-ranked woman in $m$’s\\
                \hspace{2cm}preference list to whom $m$ has not yet proposed\\
                \hspace{1cm}If $w$ is free then\\
                \hspace{2cm}$(m, w)$ become engaged\\
                \hspace{1cm}Else $w$ is currently engaged to $m'$\\
                \hspace{2cm}If $w$ prefers $m'$ to $m$ then\\
                \hspace{3cm}$m$ remains free\\
                \hspace{2cm}Else $w$ prefers $m$ to $m'$\\
                \hspace{3cm}$(m, w)$ become engaged\\
                \hspace{3cm}$m'$ becomes free\\
                \hspace{2cm}Endif\\
                \hspace{1cm}Endif\\
                Endwhile\\
                Return the set $S$ of engaged pairs.
            }
        }
    \end{frame}
    \section{Graph Basics}
    \begin{frame}{What are graphs?}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \onslide<1->{Graphs are a way to represent relations between data points.}
                \bigskip

                \onslide<2->{Graphs have a set of vertices which 
                represent data points and a set of edges to model the \textbf{relation} between vertices.}
                \bigskip

                \onslide<3->{An edge exists between two or more 
                vertices $iff$ there is a connection between them.}
            \end{column}
            \begin{column}{0.5\textwidth}
                \defineFigure
                {Graph}{0.80}
                {Sourced from Youtube}{lookatthisgraph}
            \end{column}
        \end{columns}
    \end{frame}
    \begin{frame}{Graph Representation}
        Adjacency List: Keep a $n$ length array. Vertices are indices of the array where each element of the array 
        contains a pointer to a list of adjacent vertices to the index/vertex. 
        This takes up $\Theta(n + m)$ space for $m$ edges.
        \bigskip

        Adjacency Matrix: Keep a $n\; X\; n$ matrix. Rows and columns are vertices. 
        An edge $(u,v)$ exists if the matrix at row $u$ and column $v$ 
        is not null This takes up $\Theta(n^2)$ space.
    \end{frame}
    \begin{frame}{Undirected vs. Directed Graphs}
        \onslide<1->A graph is undirected if edges go both ways. If for any vertices $u$, $v$, vertex $u$ connects to $v$, $v$ connects to $u$.
        \bigskip

        \onslide<2->A graph is directed if there is an edge $(u,v)$ such that $u$ is connected to $v$, but $v$ is not connected to $u$.
        \bigskip

        \onslide<3->{A graph is a tree if it is connected and there are \textbf{no} cycles present.}
        \bigskip

        \onslide<4->Bus transportation routes could be classified as an undirected graph, while airports and flights could be modeled as a directed graph.
    \end{frame}
    \begin{frame}{Paths and Cycles}
        \onslide<1->{A path is a sequence of vertices and edges.}
        \bigskip

        \onslide<2->{A cycle is a sequence of vertices and edges 
        where the first and last vertex are \textbf{the same}.}
        \bigskip

        \onslide<3->{A \textbf{simple path} is a path that contains 
        no cycles. A \textbf{simple cycle} is a cycle in which the only 
        repeating vertices are the first and last.}
        \bigskip

        \onslide<4->{A cycle needs \textbf{at least} 4 elements. The \textbf{maximum length} 
        of a simple path for a graph with $n$ 
        vertices is $n - 1$ \only<4->{\footnote{Pigeonhole Principle}}.}
    \end{frame}
    \begin{frame}{Connectivity}
        \only<-3>{
            \onslide<1->{A graph is connected if there is a path between any 
            two vertices.}
            \bigskip
            
            \onslide<2->{A \textbf{directed} graph is \textit{strongly connected}
             $iff$ for any to vertices $u$ and $v$, if there is 
             a path from $u$ to $v$, there must also be a path from $v$ to $u$.}
            \bigskip
    
            \onslide<3->{A directed acyclic graph $\Leftrightarrow$ topological 
            ordering, meaning the vertices can be ordered in a way that 
            all edges point in the \textbf{same} direction.}
            \bigskip  
        }
        \only<4->{
            \onslide<4->{Breadth First Search (BFS) traverses a 
            graph by layers and builds a BFS tree.}
            \bigskip

            \onslide<5->{It runs in $\Theta(n + m)$ time and is 
            linear with respect to its input size.}
            \bigskip

            \onslide<6->{Depth First Search (DFS) traverses a graph by picking a 
            point and following it until a dead end.}
            \bigskip

            \onslide<7->{It runs in $\Theta(n + m)$ time and is 
            linear with respect to its input size.}
        }
    \end{frame}
    \section{Greedy Algorithms}
    \begin{frame}{Introduction}
        \begin{columns}
            \begin{column}{0.5\textwidth}
                \onslide<1->{A greedy algorithm is an algorithm that at each step, 
                produces a \textbf{locally} optimal solution. 
                The goal is to achieve \textbf{polynomial time} 
                for a given algorithmic problem.}
                \bigskip
        
                \onslide<2->{It follows that at the end of the algorithm, the 
                locally optimal solution returned can \textbf{approximate} 
                a globally optimal solution. A greedy algorithm \textbf{never}
                backtracks.}
                \bigskip
        
                \onslide<3->{See \hyperlink<3->{def:greedystaysahead}
                {\textbf{the greedy stays ahead proof}} for details on 
                proving these types of algorithms.}       
            \end{column}
            \begin{column}{0.5\textwidth}
                \defineFigure
                {ksfmf2nhqk461}{0.75}
                {\small Sourced from Google}{greedypic}
            \end{column}
        \end{columns}
    \end{frame}
    \begin{frame}{Interval Scheduling Problem}
        \onslide<1->{Given a set of tasks to do over a period in time, how
        do you pick the \textbf{maximum} number 
        of tasks that aren't in conflict?}
        \bigskip

        \onslide<2->{One greedy solution is to sort the set of tasks by 
        their earliest $\rightarrow$ latest finishing time.}
        \bigskip

    \end{frame}
    \begin{frame}{Minimum Spanning Tree Problem}
        \onslide<1->{Given a \textbf{weighted}, graph, how do you construct a tree such that 
        the accumulation of all the weights in the tree is minimized?}
        \bigskip

        \onslide<2->{There are numerous greedy algorithms to generate an MST:}
        \begin{itemize}
            \item<3->\href{https://en.wikipedia.org/wiki/Prim\%27s\_algorithm}{Prim's Algorithm}
            \item<4->\href{https://en.wikipedia.org/wiki/Bor\%C5\%AFvka\%27s\_algorithm}{Boruvka’s Algorithm}
            \item<5->\href{https://en.wikipedia.org/wiki/Kruskal\%27s\_algorithm}{Kruskal's Algorithm}
        \end{itemize}
        \onslide<6->Refer to the sources for proof of correctness and runtime analysis.
    \end{frame}
    \begin{frame}{Shortest Path Problem}
        \onslide<1->{Given a weighted graph with \textbf{no} negative weights,
        how do you find the shortest path?}        
    \end{frame}
\end{document}